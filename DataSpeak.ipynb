{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSpeak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time,sleep\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint as rand\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "import transformers \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from gensim import models\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import logging\n",
    "import streamlit as st\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        # F1 Score\n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'F1 Score') \n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'ROC Curve')\n",
    "        \n",
    "        # PRC\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'PRC')        \n",
    "\n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    print(df_eval_stats)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned = \" \".join(re.sub(r\"[^0-9a-zA-Z']\", \" \", text).split())\n",
    "    return cleaned.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word2vec_answer(question, corpus, word2vec_model):\n",
    "    question_tokens = question.lower().split()\n",
    "    question_vector = np.mean([word2vec_model.wv[token] for token in question_tokens if token in word2vec_model.wv], axis=0)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for index in range(len(corpus)):\n",
    "        sentence = ' '.join(corpus[index])\n",
    "        sentence_tokens = sentence.lower().split()\n",
    "        sentence_vector = np.mean([word2vec_model.wv[token] for token in sentence_tokens if token in word2vec_model.wv], axis=0)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "\n",
    "    # Calculate cosine similarities between the question vector and sentence vectors\n",
    "    #print(jellyfish.levenshtein_distance(str(question), str(corpus)))\n",
    "    similarities = cosine_similarity([question_vector], sentence_vectors)[0]\n",
    "    # Find the sentence with the highest similarity as the answer\n",
    "    max_similarity_index = np.argmax(similarities)\n",
    "    print(max_similarity_index)\n",
    "    answer = corpus[max_similarity_index]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_bert_question(question,context):\n",
    "    # Load the pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "    \n",
    "    '''\n",
    "    context = generate_word2vec_answer(question,tokenized_corpus,word2vec_model)\n",
    "    final_context = []\n",
    "    for word in context:\n",
    "        if word not in ['p','gt','ul','\\n','li']:\n",
    "            final_context.append(word)\n",
    "    final_context = ' '.join(final_context)\n",
    "    '''\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    encoding = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    # Get model's output\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "    \n",
    "    # Convert start_scores and end_scores to tensors if not already\n",
    "    if not isinstance(start_scores, torch.Tensor):\n",
    "        start_scores = torch.tensor(start_scores)\n",
    "    if not isinstance(end_scores, torch.Tensor):\n",
    "        end_scores = torch.tensor(end_scores)\n",
    "    \n",
    "    # Find the answer span in the text\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    # Decode and return the answer\n",
    "    answer_tokens = input_ids[0][start_index:end_index]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(user_question, df, model, tokenizer, n=5):\n",
    "    # Calculate TF-IDF vectors for questions\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    question_tfidf = tfidf_vectorizer.fit_transform(df['Body_questions_norm'])\n",
    "\n",
    "    # Calculate the TF-IDF vector for the user question\n",
    "    user_question_tfidf = tfidf_vectorizer.transform([user_question])\n",
    "\n",
    "    # Calculate cosine similarity between user question and dataset questions\n",
    "    similarities = cosine_similarity(user_question_tfidf, question_tfidf)\n",
    "    print(sorted(similarities))\n",
    "\n",
    "    # Sort questions by similarity score\n",
    "    sorted_indices = similarities.argsort()[0][::-1]\n",
    "    print(sorted_indices)\n",
    "\n",
    "    # Get the top N answers based on similarity\n",
    "    top_answers = df['Body_answers_norm'].iloc[sorted_indices[:n]].tolist()\n",
    "    \n",
    "    final_answers = []\n",
    "    for answer in top_answers:\n",
    "        answer = rewrite_sentence(answer)\n",
    "        final_answers.append(answer)\n",
    "\n",
    "    return final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(answers, model, tokenizer):\n",
    "    tokenized_answers = tokenizer(answers, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    print(tokenized_answers['input_ids'].shape)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_answers)\n",
    "        logits = outputs.logits\n",
    "    print(logits.shape)\n",
    "    perplexity = torch.exp(torch.nn.functional.cross_entropy(logits, tokenized_answers[\"input_ids\"]))\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(word):\n",
    "    # Check if a word is considered valid\n",
    "    return len(word) > 1 or word.lower() in [\"a\", \"i\"]\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "def preserve_tense(word, new_word):\n",
    "    # Preserve the tense of the word\n",
    "    if word.endswith('ed'):\n",
    "        return new_word + 'ed'\n",
    "    elif word.endswith('ing'):\n",
    "        return new_word + 'ing'\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_new_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    rewritten_sentence = []\n",
    "    for word in words:\n",
    "        if(word.lower()=='it'):\n",
    "            rewritten_sentence.append(word)\n",
    "        else:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                # Choose a random synonym from the list\n",
    "                synonym = synonyms[0]\n",
    "                synonym = preserve_tense(word,synonym)\n",
    "                rewritten_sentence.append(synonym)\n",
    "            else:\n",
    "                rewritten_sentence.append(word)\n",
    "    return ' '.join(rewritten_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sloppy_sentence(sloppy_sentence):\n",
    "    # Split the sentence into words\n",
    "    words = sloppy_sentence.split()\n",
    "\n",
    "    # Initialize a list to store cleaned words\n",
    "    cleaned_words = []\n",
    "\n",
    "    # Flag to keep track of the first word\n",
    "    first_word = True\n",
    "\n",
    "    # Iterate through the words\n",
    "    for word in words:\n",
    "        # Check if the word is considered a valid word\n",
    "        if is_word(word):\n",
    "            # Capitalize the first letter of the cleaned sentence\n",
    "            if first_word:\n",
    "                word = word[0].upper() + word[1:]\n",
    "                first_word = False\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    # Join the cleaned words to form the cleaned sentence\n",
    "    cleaned_sentence = ' '.join(cleaned_words)\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_sentence(sentence):\n",
    "    sentence = write_new_sentence(clean_sloppy_sentence(sentence))\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Tag the words with their parts of speech\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # Initialize a list to store the rewritten words\n",
    "    rewritten_words = []\n",
    "\n",
    "    for word, pos in tagged_words:\n",
    "        # Rewrite the word while preserving tense (if applicable)\n",
    "        if pos.startswith('V'):  # Verbs\n",
    "            rewritten_word = word\n",
    "        else:\n",
    "            rewritten_word = word  # Keep non-verbs as they are\n",
    "        rewritten_words.append(rewritten_word)\n",
    "\n",
    "    # Join the rewritten words to form the rewritten sentence\n",
    "    rewritten_sentence = ' '.join(rewritten_words)\n",
    "\n",
    "    return rewritten_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Merged_QA.csv', sep=',', on_bad_lines='skip', encoding='latin-1', engine='python')\n",
    "tags = pd.read_csv('Tags.csv', sep=',', on_bad_lines='skip', encoding='latin-1', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id_question</th>\n",
       "      <th>OwnerUserId_question</th>\n",
       "      <th>Score_question</th>\n",
       "      <th>Id_answer</th>\n",
       "      <th>OwnerUserId_answer</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>Score_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>968850.000000</td>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>9.688500e+05</td>\n",
       "      <td>968850.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.133843e+05</td>\n",
       "      <td>2.065187e+07</td>\n",
       "      <td>2.189276e+06</td>\n",
       "      <td>7.184380</td>\n",
       "      <td>2.171903e+07</td>\n",
       "      <td>1.595523e+06</td>\n",
       "      <td>2.065187e+07</td>\n",
       "      <td>3.012258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.012276e+05</td>\n",
       "      <td>1.192135e+07</td>\n",
       "      <td>1.871372e+06</td>\n",
       "      <td>63.628068</td>\n",
       "      <td>1.175772e+07</td>\n",
       "      <td>1.651070e+06</td>\n",
       "      <td>1.192135e+07</td>\n",
       "      <td>21.180729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.690000e+02</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>-44.000000</td>\n",
       "      <td>4.970000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.690000e+02</td>\n",
       "      <td>-38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.519382e+05</td>\n",
       "      <td>1.013404e+07</td>\n",
       "      <td>5.676200e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.159879e+07</td>\n",
       "      <td>2.665050e+05</td>\n",
       "      <td>1.013404e+07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.065275e+05</td>\n",
       "      <td>2.116424e+07</td>\n",
       "      <td>1.651917e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.266524e+07</td>\n",
       "      <td>9.736670e+05</td>\n",
       "      <td>2.116424e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.704678e+05</td>\n",
       "      <td>3.124631e+07</td>\n",
       "      <td>3.503982e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.213461e+07</td>\n",
       "      <td>2.445413e+06</td>\n",
       "      <td>3.124631e+07</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.055161e+06</td>\n",
       "      <td>4.014319e+07</td>\n",
       "      <td>7.044992e+06</td>\n",
       "      <td>5524.000000</td>\n",
       "      <td>4.014337e+07</td>\n",
       "      <td>7.044747e+06</td>\n",
       "      <td>4.014319e+07</td>\n",
       "      <td>8384.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0   Id_question  OwnerUserId_question  Score_question  \\\n",
       "count  9.688500e+05  9.688500e+05          9.688500e+05   968850.000000   \n",
       "mean   5.133843e+05  2.065187e+07          2.189276e+06        7.184380   \n",
       "std    3.012276e+05  1.192135e+07          1.871372e+06       63.628068   \n",
       "min    0.000000e+00  4.690000e+02          2.500000e+01      -44.000000   \n",
       "25%    2.519382e+05  1.013404e+07          5.676200e+05        0.000000   \n",
       "50%    5.065275e+05  2.116424e+07          1.651917e+06        1.000000   \n",
       "75%    7.704678e+05  3.124631e+07          3.503982e+06        3.000000   \n",
       "max    1.055161e+06  4.014319e+07          7.044992e+06     5524.000000   \n",
       "\n",
       "          Id_answer  OwnerUserId_answer      ParentId   Score_answer  \n",
       "count  9.688500e+05        9.688500e+05  9.688500e+05  968850.000000  \n",
       "mean   2.171903e+07        1.595523e+06  2.065187e+07       3.012258  \n",
       "std    1.175772e+07        1.651070e+06  1.192135e+07      21.180729  \n",
       "min    4.970000e+02        1.000000e+00  4.690000e+02     -38.000000  \n",
       "25%    1.159879e+07        2.665050e+05  1.013404e+07       0.000000  \n",
       "50%    2.266524e+07        9.736670e+05  2.116424e+07       1.000000  \n",
       "75%    3.213461e+07        2.445413e+06  3.124631e+07       3.000000  \n",
       "max    4.014337e+07        7.044747e+06  4.014319e+07    8384.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1885078 entries, 0 to 1885077\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   Id      int64 \n",
      " 1   Tag     object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 28.8+ MB\n"
     ]
    }
   ],
   "source": [
    "tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[(data['Score_question'] >= 0) & (data['Score_question'] <= 10) & (data['Score_answer'] >=0) & (data['Score_answer'] <= 10)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 805615 entries, 0 to 805614\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Unnamed: 0            805615 non-null  int64  \n",
      " 1   Id_question           805615 non-null  int64  \n",
      " 2   OwnerUserId_question  805615 non-null  float64\n",
      " 3   Score_question        805615 non-null  int64  \n",
      " 4   Title                 805615 non-null  object \n",
      " 5   Id_answer             805615 non-null  float64\n",
      " 6   OwnerUserId_answer    805615 non-null  float64\n",
      " 7   ParentId              805615 non-null  float64\n",
      " 8   Score_answer          805615 non-null  float64\n",
      " 9   Body_questions_norm   805615 non-null  object \n",
      " 10  Body_answers_norm     805615 non-null  object \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 67.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id_question</th>\n",
       "      <th>OwnerUserId_question</th>\n",
       "      <th>Score_question</th>\n",
       "      <th>Title</th>\n",
       "      <th>Id_answer</th>\n",
       "      <th>OwnerUserId_answer</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>Score_answer</th>\n",
       "      <th>Body_questions_norm</th>\n",
       "      <th>Body_answers_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95608</th>\n",
       "      <td>151016</td>\n",
       "      <td>5546072</td>\n",
       "      <td>679232.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a List from a Binary Search Tree</td>\n",
       "      <td>5546129.0</td>\n",
       "      <td>675568.0</td>\n",
       "      <td>5546072.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>p i'm trying to make a list of all items in a ...</td>\n",
       "      <td>p code inorder code prints things but does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235370</th>\n",
       "      <td>327503</td>\n",
       "      <td>13478596</td>\n",
       "      <td>1608226.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Sorting function for a list of points based on...</td>\n",
       "      <td>13478738.0</td>\n",
       "      <td>1357341.0</td>\n",
       "      <td>13478596.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>p question has been rewritten for clarificatio...</td>\n",
       "      <td>p well using the average seems like the intuit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Id_question  OwnerUserId_question  Score_question  \\\n",
       "95608       151016      5546072              679232.0               4   \n",
       "235370      327503     13478596             1608226.0               2   \n",
       "\n",
       "                                                    Title   Id_answer  \\\n",
       "95608           Creating a List from a Binary Search Tree   5546129.0   \n",
       "235370  Sorting function for a list of points based on...  13478738.0   \n",
       "\n",
       "        OwnerUserId_answer    ParentId  Score_answer  \\\n",
       "95608             675568.0   5546072.0           1.0   \n",
       "235370           1357341.0  13478596.0           2.0   \n",
       "\n",
       "                                      Body_questions_norm  \\\n",
       "95608   p i'm trying to make a list of all items in a ...   \n",
       "235370  p question has been rewritten for clarificatio...   \n",
       "\n",
       "                                        Body_answers_norm  \n",
       "95608   p code inorder code prints things but does not...  \n",
       "235370  p well using the average seems like the intuit...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Score_question'].plot(kind='hist',\n",
    "                          bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Score_answer'].plot(kind='hist',\n",
    "                        bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['Body'].replace(['<p>','</p>','\\n','<ul>','<li>','</li>','</ul>'],'',regex=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "for index in range(10000):\n",
    "    val = str(df['Body_questions_norm'][index])\n",
    "    if(index%50000==0):\n",
    "        print(index)\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(width = 800, height = 800,\n",
    "                        background_color ='white',\n",
    "                        stopwords = stopwords,\n",
    "                        min_font_size = 10).generate(comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Body_questions_norm'],df['Body_answers_norm'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = X_train\n",
    "tokenized_corpus_train = [sentence.lower().split() for sentence in corpus_train[:100000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = X_test\n",
    "tokenized_corpus_test = [sentence.lower().split() for sentence in corpus_test[:100000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = models.Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.build_vocab(tokenized_corpus,progress_per=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = bert_preprocess_model(X_train[:100])\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(encoder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Body_questions_norm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Body_answers_norm'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183c947284c94052a9e54fff3b1ac1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347c54cdebb249e68ffdcb502c66735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bc5a85df8346d19c65b91277434d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd22ffe2053147bb86d3054e2c966d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = str(input(\"Ask a question (type 'exit' to quit): \"))\n",
    "    os.system('cls')\n",
    "    if(question.lower() in ['exit','done','none','goodbye','bye','finished']):\n",
    "        break\n",
    "    generated_answers = generate_answers(user_question, df, model, tokenizer, n=5)\n",
    "    for i, answer in enumerate(generated_answers):\n",
    "        answer = rewrite_sentence(answer)\n",
    "        print(f\"Answer {i + 1}: {answer}\\n\")\n",
    "    #perplexity = calculate_perplexity(generated_answers, model, tokenizer)\n",
    "    #print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 15:48:28.175 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\michael\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "st.header(\"BERT Question and Answer\")\n",
    "st.write(\"\"\"\n",
    "         Ask any question and I will give you 5 possible answers.\n",
    "         \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = st.text_input(\"\")\n",
    "generated_answers = generate_answers(user_question, df, model, tokenizer, n=5)\n",
    "st.write(generated_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
